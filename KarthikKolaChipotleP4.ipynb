{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ed931b-b1b5-43fe-aae6-aace5d6edc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for each state:\n",
      "   region  Focal_Chain_Observations  Benchmark_Chain_Observations\n",
      "0      AZ                      1813                          1813\n",
      "1      CA                       917                           917\n",
      "2      CO                      1827                          1827\n",
      "3      CT                       294                           294\n",
      "4      DC                       938                           938\n",
      "5      DE                      1827                          1827\n",
      "6      FL                      1827                          1827\n",
      "7      GA                      1204                          1204\n",
      "8      IL                      1827                          1827\n",
      "9      IN                      1827                          1827\n",
      "10     KS                      1771                          1771\n",
      "11     KY                      1827                          1827\n",
      "12     LA                       518                           518\n",
      "13     MA                      1827                          1827\n",
      "14     MD                      1827                          1827\n",
      "15     MI                      1827                          1827\n",
      "16     MN                      1827                          1827\n",
      "17     MO                      1827                          1827\n",
      "18     NC                      1827                          1827\n",
      "19     ND                      1113                          1113\n",
      "20     NE                      1827                          1827\n",
      "21     NJ                      1827                          1827\n",
      "22     NV                      1827                          1827\n",
      "23     NY                      1827                          1827\n",
      "24     OK                      1827                          1827\n",
      "25     OR                      1827                          1827\n",
      "26     PA                      1827                          1827\n",
      "27     SC                       546                           546\n",
      "28     TN                      1344                          1344\n",
      "29     TX                      1827                          1827\n",
      "30     VA                      1827                          1827\n",
      "31     WA                      1827                          1827\n",
      "32     WI                      1827                          1827\n",
      "33     WV                       707                           707\n"
     ]
    }
   ],
   "source": [
    "# Load the MergedSummary CSV created at the end of Project 3\n",
    "import pandas as pd\n",
    "\n",
    "merged_summary = pd.read_csv('MergedSummary.csv')\n",
    "\n",
    "# Group by state (region) and calculate the number of observations for each chain\n",
    "state_observations = merged_summary.groupby('region').agg(\n",
    "    Focal_Chain_Observations=('SumDailyVisits_Focal', 'count'),\n",
    "    Benchmark_Chain_Observations=('SumDailyVisits_Benchmark', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Display the table\n",
    "print(\"Number of observations for each state:\")\n",
    "print(state_observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016668e-b605-42ff-9910-663e449d12c5",
   "metadata": {},
   "source": [
    "The observations are the same since the merged set from Project 3 is what is inclusive of both, Qdoba having data only in 33 states, so we use the lower number (Chipotle is in 46)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a82fb-1825-436b-9633-d5be603c876a",
   "metadata": {},
   "source": [
    "1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e643c9-8eff-4805-b272-3fdea117aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state-level dataset saved as 'Final_Analysis_Sample.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the required datasets\n",
    "merged_summary = pd.read_csv('MergedSummary.csv')  # This contains state-level aggregated data\n",
    "census_demographics = pd.read_csv('census_state_2016.csv')  # This contains state-level demographic data\n",
    "\n",
    "# Perform a state-level merge\n",
    "final_analysis_sample = pd.merge(\n",
    "    merged_summary,\n",
    "    census_demographics,\n",
    "    left_on='region',        # State in MergedSummary\n",
    "    right_on='state_abbr',   # State abbreviation in Census data\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Ensure we group by state-level observations\n",
    "final_analysis_sample_grouped = final_analysis_sample.groupby('region').first().reset_index()\n",
    "\n",
    "# Save the final dataset for further analysis\n",
    "final_analysis_sample_grouped.to_csv('Final_Analysis_Sample.csv', index=False)\n",
    "print(\"Final state-level dataset saved as 'Final_Analysis_Sample.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f945770-19f8-4270-afa9-f217ad365cc7",
   "metadata": {},
   "source": [
    "1.b \n",
    "Number of observations in MergedSummary: 34 (unique states)\n",
    "Number of observations in Census_demographics: 52 (unique states)\n",
    "Number of matched observations in Final_analysis_sample: 34 (states)\n",
    "Number of observations in MergedSummary but not in Census_demographics: 0\n",
    "Number of observations in Census_demographics but not in MergedSummary: 18\n",
    "\n",
    "The imperfect matches come from the fact that Chipotle only really operates in 34 unique places that are in conjunction with the benchmark Qdoba, the census data has information from all the states so its ging to be a larger number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9e0d1f-9ea8-45b8-83b4-fe33ae979986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Final Analysis Sample with Day-of-Week Indicators and Log Transforms:\n",
      "  region   new_date  Nunits_Benchmark  SumDailyVisits_Benchmark  \\\n",
      "0     AZ 2018-01-01               1.0                       2.0   \n",
      "1     AZ 2018-01-02               1.0                       2.0   \n",
      "2     AZ 2018-01-03               1.0                      10.0   \n",
      "3     AZ 2018-01-04               1.0                       4.0   \n",
      "4     AZ 2018-01-05               1.0                       5.0   \n",
      "\n",
      "   MaxDailyVisitsPerUnit_Benchmark  MinDailyVisitsPerUnit_Benchmark  \\\n",
      "0                              2.0                              2.0   \n",
      "1                              2.0                              2.0   \n",
      "2                             10.0                             10.0   \n",
      "3                              4.0                              4.0   \n",
      "4                              5.0                              5.0   \n",
      "\n",
      "   MedDailyVisitsPerUnit_Benchmark  Nunits_Focal  SumDailyVisits_Focal  \\\n",
      "0                              2.0          19.0                  67.0   \n",
      "1                              2.0          19.0                  86.0   \n",
      "2                             10.0          19.0                 114.0   \n",
      "3                              4.0          19.0                 120.0   \n",
      "4                              5.0          19.0                 121.0   \n",
      "\n",
      "   MaxDailyVisitsPerUnit_Focal  ...  day_of_week  BlueState  \\\n",
      "0                          7.0  ...       Monday          0   \n",
      "1                         11.0  ...      Tuesday          0   \n",
      "2                         17.0  ...    Wednesday          0   \n",
      "3                         14.0  ...     Thursday          0   \n",
      "4                         15.0  ...       Friday          0   \n",
      "\n",
      "  LogTotalPopulation  LogHispanicLatino  day_Monday  day_Saturday  day_Sunday  \\\n",
      "0          15.751525          14.578546        True         False       False   \n",
      "1          15.751525          14.578546       False         False       False   \n",
      "2          15.751525          14.578546       False         False       False   \n",
      "3          15.751525          14.578546       False         False       False   \n",
      "4          15.751525          14.578546       False         False       False   \n",
      "\n",
      "   day_Thursday  day_Tuesday  day_Wednesday  \n",
      "0         False        False          False  \n",
      "1         False         True          False  \n",
      "2         False        False           True  \n",
      "3          True        False          False  \n",
      "4         False        False          False  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Ensure the 'new_date' column is in datetime format for extracting day of the week\n",
    "final_analysis_sample['new_date'] = pd.to_datetime(final_analysis_sample['new_date'])\n",
    "\n",
    "# Create Day of Week Indicators\n",
    "final_analysis_sample['day_of_week'] = final_analysis_sample['new_date'].dt.day_name()\n",
    "day_dummies = pd.get_dummies(final_analysis_sample['day_of_week'], prefix='day', drop_first=True)\n",
    "\n",
    "# Include 'BlueState' Variable (already defined in Project 3)\n",
    "blue_states = ['CA', 'NY', 'WA', 'MA', 'MD', 'IL', 'NJ', 'VT', 'RI', 'CT', 'DE', 'HI', 'OR', 'NM', 'NV']\n",
    "final_analysis_sample['BlueState'] = final_analysis_sample['region'].apply(lambda x: 1 if x in blue_states else 0)\n",
    "\n",
    "# Log Transform the Population and Hispanic/Latino Variables\n",
    "final_analysis_sample['LogTotalPopulation'] = np.log(final_analysis_sample['total_population'] + 1)\n",
    "final_analysis_sample['LogHispanicLatino'] = np.log(final_analysis_sample['hispanic_latino'] + 1)\n",
    "\n",
    "# Add the day-of-week dummies to the dataset\n",
    "final_analysis_sample = pd.concat([final_analysis_sample, day_dummies], axis=1)\n",
    "\n",
    "# Save the updated dataset for further analysis\n",
    "final_analysis_sample.to_csv('Updated_Final_Analysis_Sample.csv', index=False)\n",
    "\n",
    "# Display a preview of the dataset\n",
    "print(\"Updated Final Analysis Sample with Day-of-Week Indicators and Log Transforms:\")\n",
    "print(final_analysis_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b314cfdb-e0f9-4774-9b96-17956a39c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between LogHispanicLatino and BlueState:\n",
      "                   LogHispanicLatino  BlueState\n",
      "LogHispanicLatino            1.00000    0.22871\n",
      "BlueState                    0.22871    1.00000\n"
     ]
    }
   ],
   "source": [
    "# Check correlation between LogHispanicLatino and BlueState\n",
    "correlation = final_analysis_sample[['LogHispanicLatino', 'BlueState']].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation between LogHispanicLatino and BlueState:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068875f-3dd3-495d-b934-2c7cb8625233",
   "metadata": {},
   "source": [
    "The correlation between Being Hispanic_latino is not that strong with Blue states so we are not too worried with using this in conjunction. We can see impact using VIF or tolerance later on in the regression part, but using Ridge and lasso can help mute this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200670e7-8daa-4221-88bf-456a9afe5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Numerical Variables:\n",
      "                      Count          Mean        Median       Std Dev  \\\n",
      "LogTotalPopulation  53186.0  1.562239e+01  1.570758e+01  8.514909e-01   \n",
      "LogHispanicLatino   53186.0  1.337541e+01  1.328232e+01  1.295267e+00   \n",
      "median_age          53186.0  3.817867e+01  3.840000e+01  1.763516e+00   \n",
      "housing_units       53186.0  3.539664e+06  2.854595e+06  2.824860e+06   \n",
      "total_population    53186.0  8.421578e+06  6.633053e+06  7.253647e+06   \n",
      "\n",
      "                              Min           Max  \n",
      "LogTotalPopulation      13.431569  1.748546e+01  \n",
      "LogHispanicLatino       10.182444  1.654211e+01  \n",
      "median_age              33.900000  4.230000e+01  \n",
      "housing_units       313703.000000  1.406138e+07  \n",
      "total_population    681170.000000  3.925002e+07  \n",
      "\n",
      "Summary of Categorical Variable (BlueState):\n",
      "   BlueState  Frequency\n",
      "0          0      35532\n",
      "1          1      17654\n",
      "\n",
      "Summary of Day-of-Week Indicators:\n",
      "  Day of Week Indicator                                          Frequency\n",
      "0           day_of_week  MondayTuesdayWednesdayThursdayFridaySaturdaySu...\n",
      "1            day_Monday                                               7598\n",
      "2          day_Saturday                                               7598\n",
      "3            day_Sunday                                               7598\n",
      "4          day_Thursday                                               7598\n",
      "5           day_Tuesday                                               7598\n",
      "6         day_Wednesday                                               7598\n"
     ]
    }
   ],
   "source": [
    "# List of numerical variables to summarize\n",
    "numerical_vars = ['LogTotalPopulation', 'LogHispanicLatino', 'median_age', 'housing_units', 'total_population']\n",
    "\n",
    "# Summarize numerical variables\n",
    "numerical_summary = final_analysis_sample[numerical_vars].describe().T\n",
    "numerical_summary['Median'] = final_analysis_sample[numerical_vars].median()\n",
    "numerical_summary = numerical_summary.rename(columns={\n",
    "    'count': 'Count', 'mean': 'Mean', 'std': 'Std Dev', 'min': 'Min', 'max': 'Max'\n",
    "})[['Count', 'Mean', 'Median', 'Std Dev', 'Min', 'Max']]\n",
    "\n",
    "print(\"Summary of Numerical Variables:\")\n",
    "print(numerical_summary)\n",
    "\n",
    "# List of categorical variables to summarize\n",
    "categorical_vars = ['BlueState']  # Add any new categorical variables to this list\n",
    "\n",
    "# Summary of categorical variables\n",
    "for var in categorical_vars:\n",
    "    cat_summary = final_analysis_sample[var].value_counts().reset_index()\n",
    "    cat_summary.columns = [var, 'Frequency']\n",
    "    print(f\"\\nSummary of Categorical Variable ({var}):\")\n",
    "    print(cat_summary)\n",
    "\n",
    "# Frequency distribution for day-of-week indicators\n",
    "day_dummies = [col for col in final_analysis_sample.columns if col.startswith('day_')]\n",
    "day_dummies_summary = final_analysis_sample[day_dummies].sum().reset_index()\n",
    "day_dummies_summary.columns = ['Day of Week Indicator', 'Frequency']\n",
    "\n",
    "print(\"\\nSummary of Day-of-Week Indicators:\")\n",
    "print(day_dummies_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912267dc-4ad4-4593-953d-760a8769c96f",
   "metadata": {},
   "source": [
    "1.c\n",
    "\n",
    "Ended up adding housing_units, this is a proxy to how urban/developed the area might be, chains like Chipotle and Qdoba are more likely to be their. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14488c18-6c9f-4b15-ba99-4e9307f1f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                  total_population  hispanic_latino\n",
       " total_population          1.000000         0.920528\n",
       " hispanic_latino           0.920528         1.000000,\n",
       " count    53186.000000\n",
       " mean         0.133215\n",
       " std          0.093905\n",
       " min          0.014627\n",
       " 25%          0.066973\n",
       " 50%          0.102951\n",
       " 75%          0.189798\n",
       " max          0.390605\n",
       " Name: hispanic_to_population_ratio, dtype: float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the final dataset for analysis\n",
    "final_analysis_sample = pd.read_csv('Updated_Final_Analysis_Sample.csv')\n",
    "\n",
    "# Compute the correlation between 'total_population' and 'hispanic_latino'\n",
    "correlation = final_analysis_sample[['total_population', 'hispanic_latino']].corr()\n",
    "\n",
    "# Compute the ratio of 'hispanic_latino' to 'total_population'\n",
    "final_analysis_sample['hispanic_to_population_ratio'] = (\n",
    "    final_analysis_sample['hispanic_latino'] / final_analysis_sample['total_population']\n",
    ")\n",
    "\n",
    "# Summary statistics for the ratio\n",
    "ratio_summary = final_analysis_sample['hispanic_to_population_ratio'].describe()\n",
    "\n",
    "correlation, ratio_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079a0c71-b11d-4870-94dd-a81a83019dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State  HispanicLatino_Proportion\n",
      "0     AZ                   0.309444\n",
      "1     CA                   0.389319\n",
      "2     CO                   0.213195\n",
      "3     CT                   0.157236\n",
      "4     DC                   0.109256\n",
      "5     DE                   0.091540\n",
      "6     FL                   0.248713\n",
      "7     GA                   0.093337\n",
      "8     IL                   0.170198\n",
      "9     IN                   0.067552\n",
      "10    KS                   0.115884\n",
      "11    KY                   0.034240\n",
      "12    LA                   0.049377\n",
      "13    MA                   0.114463\n",
      "14    MD                   0.097521\n",
      "15    MI                   0.049498\n",
      "16    MN                   0.052258\n",
      "17    MO                   0.039975\n",
      "18    NC                   0.091764\n",
      "19    ND                   0.034876\n",
      "20    NE                   0.106155\n",
      "21    NJ                   0.199751\n",
      "22    NV                   0.284561\n",
      "23    NY                   0.189798\n",
      "24    OK                   0.102951\n",
      "25    OR                   0.127659\n",
      "26    PA                   0.070399\n",
      "27    SC                   0.054986\n",
      "28    TN                   0.052238\n",
      "29    TX                   0.390605\n",
      "30    VA                   0.090422\n",
      "31    WA                   0.124376\n",
      "32    WI                   0.066973\n",
      "33    WV                   0.014627\n"
     ]
    }
   ],
   "source": [
    "# Calculate the proportion of Hispanic/Latino population for each state\n",
    "final_analysis_sample['HispanicLatino_Proportion'] = (\n",
    "    final_analysis_sample['hispanic_latino'] / final_analysis_sample['total_population']\n",
    ")\n",
    "\n",
    "# Group by state to calculate the average proportion for each state\n",
    "state_hispanic_latino_proportion = final_analysis_sample.groupby('region')['HispanicLatino_Proportion'].mean().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "state_hispanic_latino_proportion.columns = ['State', 'HispanicLatino_Proportion']\n",
    "\n",
    "# Display the results\n",
    "print(state_hispanic_latino_proportion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c6edc-4816-45a0-94c8-eb6043cff888",
   "metadata": {},
   "source": [
    "Decided to remove total population, and just use hispanic_latino, they are higly correlated, but since the chain and benchmark chain is based around Mexican food, we are going to prioritize this var as it has alot of information that total population has and a potentially a bit more in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98965d58-c5e8-432f-8c6e-4bc2bf2e1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample Summary Before Scaling:\n",
      "       LogHispanicLatino     BlueState    median_age  housing_units\n",
      "count       37230.000000  37230.000000  37230.000000   3.723000e+04\n",
      "mean           13.381751      0.331319     38.177094   3.560557e+06\n",
      "std             1.296656      0.470694      1.764395   2.838633e+06\n",
      "min            10.182444      0.000000     33.900000   3.137030e+05\n",
      "25%            12.727571      0.000000     36.700000   1.732887e+06\n",
      "50%            13.282322      0.000000     38.400000   2.854595e+06\n",
      "75%            13.982058      1.000000     39.400000   4.540697e+06\n",
      "max            16.542106      1.000000     42.300000   1.406138e+07\n",
      "\n",
      "Test Sample Summary Before Scaling:\n",
      "       LogHispanicLatino     BlueState    median_age  housing_units\n",
      "count       15956.000000  15956.000000  15956.000000   1.595600e+04\n",
      "mean           13.360626      0.333354     38.182333   3.490916e+06\n",
      "std             1.291940      0.471427      1.761513   2.791939e+06\n",
      "min            10.182444      0.000000     33.900000   3.137030e+05\n",
      "25%            12.727571      0.000000     36.700000   1.732887e+06\n",
      "50%            13.282322      0.000000     38.400000   2.854595e+06\n",
      "75%            13.982058      1.000000     39.400000   4.540697e+06\n",
      "max            16.542106      1.000000     42.300000   1.406138e+07\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the updated dataset\n",
    "data = pd.read_csv('Updated_Final_Analysis_Sample.csv')\n",
    "\n",
    "# Drop any rows with missing values in the features or target variable\n",
    "data = data.dropna(subset=[\n",
    "    'LogHispanicLatino', \n",
    "    'BlueState', \n",
    "    'median_age',\n",
    "    'housing_units',\n",
    "    'total_population',\n",
    "    'day_Monday', \n",
    "    'day_Tuesday', \n",
    "    'day_Wednesday', \n",
    "    'day_Thursday', \n",
    "    'day_Saturday',\n",
    "    'SumDailyVisits_Focal',\n",
    "    'SumDailyVisits_Benchmark'\n",
    "])\n",
    "\n",
    "# Compute the dependent variable (Y)\n",
    "data['Y'] = np.log(data['SumDailyVisits_Focal'] + 1) - np.log(data['SumDailyVisits_Benchmark'] + 1)\n",
    "\n",
    "# Define features used in the model\n",
    "features = [\n",
    "    'LogHispanicLatino', \n",
    "    'BlueState', \n",
    "    'median_age',\n",
    "    'housing_units',\n",
    "    'day_Monday', \n",
    "    'day_Tuesday', \n",
    "    'day_Wednesday', \n",
    "    'day_Thursday', \n",
    "    'day_Saturday'\n",
    "]\n",
    "\n",
    "# Select the features and target variable\n",
    "X = data[features]\n",
    "y = data['Y']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Generate summary statistics before scaling\n",
    "print(\"Training Sample Summary Before Scaling:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "print(\"\\nTest Sample Summary Before Scaling:\")\n",
    "print(X_test.describe())\n",
    "\n",
    "# Standardize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a055e64-0b95-425b-b16a-630f5bffca86",
   "metadata": {},
   "source": [
    "2.a \n",
    "\n",
    "Number of observations in the training sample: 37230\n",
    "Number of observations in the test sample: 15956 \n",
    "\n",
    "Housing Units Difference: The test sample has, on average, 69,641 fewer housing units than the training sample.\n",
    "Total Population Difference: The test sample has, on average, 174,688 fewer people than the training sample.\n",
    "\n",
    "The differnces overall for all the features are not quit significant just by looking at the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d6aaf2-0a25-4bdd-835a-617ca58e588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.283\n",
      "Model:                            OLS   Adj. R-squared:                  0.283\n",
      "Method:                 Least Squares   F-statistic:                     1632.\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        21:10:36   Log-Likelihood:                -65058.\n",
      "No. Observations:               37230   AIC:                         1.301e+05\n",
      "Df Residuals:                   37220   BIC:                         1.302e+05\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                -0.5914      0.033    -18.053      0.000      -0.656      -0.527\n",
      "LogHispanicLatino     2.2502      0.070     32.113      0.000       2.113       2.388\n",
      "BlueState             0.5792      0.017     34.333      0.000       0.546       0.612\n",
      "median_age            0.3264      0.036      9.155      0.000       0.257       0.396\n",
      "housing_units         1.7076      0.067     25.413      0.000       1.576       1.839\n",
      "day_Monday           -0.0361      0.023     -1.544      0.123      -0.082       0.010\n",
      "day_Tuesday          -0.0185      0.023     -0.795      0.427      -0.064       0.027\n",
      "day_Wednesday        -0.0299      0.023     -1.283      0.199      -0.076       0.016\n",
      "day_Thursday         -0.0419      0.023     -1.799      0.072      -0.088       0.004\n",
      "day_Saturday          0.0261      0.023      1.118      0.263      -0.020       0.072\n",
      "==============================================================================\n",
      "Omnibus:                      876.956   Durbin-Watson:                   2.009\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1548.081\n",
      "Skew:                          -0.198   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.917   Cond. No.                         18.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Reset indices of X_train_scaled_df and y_train\n",
    "X_train_scaled_df = X_train_scaled_df.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Add a constant term for the intercept to the training data\n",
    "X_train_scaled_df = sm.add_constant(X_train_scaled_df)\n",
    "\n",
    "# Fit the linear regression model using statsmodels on the training data\n",
    "model = sm.OLS(y_train, X_train_scaled_df)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9aea429-39fe-4935-85d9-39a151494150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between latinohispanic and housing_units: 0.8821\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation coefficient\n",
    "correlation = data['hispanic_latino'].corr(data['housing_units'])\n",
    "\n",
    "print(f\"Correlation between latinohispanic and housing_units: {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f853c8-9823-4ad7-b899-dac950121512",
   "metadata": {},
   "source": [
    "Re-splitting and running regression after removing housing_units, as its very correlated. Wanted to experiment where the variation explained from Housing_unit would go too, and it got observed into LogHispanicLatino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0092059e-5b3e-4600-b8c5-33e1b640432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample Summary Before Scaling:\n",
      "       LogHispanicLatino     BlueState    median_age\n",
      "count       37230.000000  37230.000000  37230.000000\n",
      "mean           13.381751      0.331319     38.177094\n",
      "std             1.296656      0.470694      1.764395\n",
      "min            10.182444      0.000000     33.900000\n",
      "25%            12.727571      0.000000     36.700000\n",
      "50%            13.282322      0.000000     38.400000\n",
      "75%            13.982058      1.000000     39.400000\n",
      "max            16.542106      1.000000     42.300000\n",
      "\n",
      "Test Sample Summary Before Scaling:\n",
      "       LogHispanicLatino     BlueState    median_age\n",
      "count       15956.000000  15956.000000  15956.000000\n",
      "mean           13.360626      0.333354     38.182333\n",
      "std             1.291940      0.471427      1.761513\n",
      "min            10.182444      0.000000     33.900000\n",
      "25%            12.727571      0.000000     36.700000\n",
      "50%            13.282322      0.000000     38.400000\n",
      "75%            13.982058      1.000000     39.400000\n",
      "max            16.542106      1.000000     42.300000\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the updated dataset\n",
    "data = pd.read_csv('Updated_Final_Analysis_Sample.csv')\n",
    "\n",
    "# Drop any rows with missing values in the features or target variable\n",
    "data = data.dropna(subset=[\n",
    "    'LogHispanicLatino', \n",
    "    'BlueState', \n",
    "    'median_age',\n",
    "    'housing_units',\n",
    "    'total_population',\n",
    "    'day_Monday', \n",
    "    'day_Tuesday', \n",
    "    'day_Wednesday', \n",
    "    'day_Thursday', \n",
    "    'day_Saturday',\n",
    "    'SumDailyVisits_Focal',\n",
    "    'SumDailyVisits_Benchmark'\n",
    "])\n",
    "\n",
    "# Compute the dependent variable (Y)\n",
    "data['Y'] = np.log(data['SumDailyVisits_Focal'] + 1) - np.log(data['SumDailyVisits_Benchmark'] + 1)\n",
    "\n",
    "# Define features used in the model\n",
    "features = [\n",
    "    'LogHispanicLatino', \n",
    "    'BlueState', \n",
    "    'median_age',\n",
    "    'day_Monday', \n",
    "    'day_Tuesday', \n",
    "    'day_Wednesday', \n",
    "    'day_Thursday', \n",
    "    'day_Saturday'\n",
    "]\n",
    "\n",
    "# Select the features and target variable\n",
    "X = data[features]\n",
    "y = data['Y']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Generate summary statistics before scaling\n",
    "print(\"Training Sample Summary Before Scaling:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "print(\"\\nTest Sample Summary Before Scaling:\")\n",
    "print(X_test.describe())\n",
    "\n",
    "# Standardize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ca47ce4-532c-4ca0-b10c-4b98130e3e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.271\n",
      "Model:                            OLS   Adj. R-squared:                  0.270\n",
      "Method:                 Least Squares   F-statistic:                     1726.\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        22:29:15   Log-Likelihood:                -65378.\n",
      "No. Observations:               37230   AIC:                         1.308e+05\n",
      "Df Residuals:                   37221   BIC:                         1.309e+05\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                -0.9891      0.029    -34.077      0.000      -1.046      -0.932\n",
      "LogHispanicLatino     3.7716      0.037    102.670      0.000       3.700       3.844\n",
      "BlueState             0.4531      0.016     27.859      0.000       0.421       0.485\n",
      "median_age            0.4766      0.035     13.439      0.000       0.407       0.546\n",
      "day_Monday           -0.0335      0.024     -1.420      0.156      -0.080       0.013\n",
      "day_Tuesday          -0.0174      0.024     -0.738      0.461      -0.063       0.029\n",
      "day_Wednesday        -0.0288      0.024     -1.225      0.220      -0.075       0.017\n",
      "day_Thursday         -0.0397      0.024     -1.687      0.092      -0.086       0.006\n",
      "day_Saturday          0.0258      0.024      1.098      0.272      -0.020       0.072\n",
      "==============================================================================\n",
      "Omnibus:                      626.825   Durbin-Watson:                   2.009\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1085.024\n",
      "Skew:                          -0.135   Prob(JB):                    2.46e-236\n",
      "Kurtosis:                       3.792   Cond. No.                         8.58\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Reset indices of X_train_scaled_df and y_train\n",
    "X_train_scaled_df = X_train_scaled_df.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Add a constant term for the intercept to the training data\n",
    "X_train_scaled_df = sm.add_constant(X_train_scaled_df)\n",
    "\n",
    "# Fit the linear regression model using statsmodels on the training data\n",
    "model = sm.OLS(y_train, X_train_scaled_df)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021490bd-6143-4ac2-9810-1dac7d1c29f1",
   "metadata": {},
   "source": [
    "2.b\n",
    " \n",
    " R-squared:                       0.271\n",
    " Adj. R-squared:                  0.270\n",
    " \n",
    " We report low predictive power in this model being at rouhgly 0.270 (adjusted r^2)\n",
    "\n",
    "    'LogHispanicLatino'is significant and has a positive effect\n",
    "    'BlueState', is significant and has a positive effect\n",
    "    'median_age', is significant and has a positive effect\n",
    "    'housing_units', is significant and has a positive effect\n",
    "    'day_Monday', is not  signifcant and has a negative effect\n",
    "    'day_Tuesday', is not  signifcant and has a negative effect\n",
    "    'day_Wednesday', is not  signifcant and has a negative effect\n",
    "    'day_Thursday', is almost signifcant and has a negative effect\n",
    "    'day_Saturday'is not  signifcant and has a positive effect\n",
    "\n",
    "    It does make sense that Blue state has an effect, we saw it had an effect in project 3. HispanicLatino does make sense that it has a big positive effect considering these chains market themselves as \"authentic\" mexic food to some extent. HispanicLatino also has high correlation with total_population and Housing_units, which are significant if added to this, that variable has cofounding information (ie if their are more people, their will be more HispanicLatinos and housing_units) but it was a suprice by how much. Age made sense considering, younger people are in theory more likely to eat at these places.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe6b91f9-55ba-41a3-b741-cd971ad21bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1e-05\n",
      "Features kept: 8, R-squared (training): 0.271, R-squared (test): 0.262\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\tLogHispanicLatino, 3.7713\n",
      "\tmedian_age, 0.4764\n",
      "\tBlueState, 0.4531\n",
      "\tday_Thursday, -0.0395\n",
      "\tday_Monday, -0.0333\n",
      "\tday_Wednesday, -0.0286\n",
      "\tday_Saturday, 0.0259\n",
      "\tday_Tuesday, -0.0172\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 0.0001\n",
      "Features kept: 8, R-squared (training): 0.271, R-squared (test): 0.262\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\tLogHispanicLatino, 3.7691\n",
      "\tmedian_age, 0.4743\n",
      "\tBlueState, 0.4531\n",
      "\tday_Thursday, -0.0379\n",
      "\tday_Monday, -0.0317\n",
      "\tday_Wednesday, -0.0270\n",
      "\tday_Saturday, 0.0262\n",
      "\tday_Tuesday, -0.0156\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 0.001\n",
      "Features kept: 7, R-squared (training): 0.270, R-squared (test): 0.261\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\tLogHispanicLatino, 3.7467\n",
      "\tBlueState, 0.4532\n",
      "\tmedian_age, 0.4532\n",
      "\tday_Saturday, 0.0293\n",
      "\tday_Thursday, -0.0223\n",
      "\tday_Monday, -0.0160\n",
      "\tday_Wednesday, -0.0114\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 0.01\n",
      "Features kept: 3, R-squared (training): 0.269, R-squared (test): 0.260\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\tLogHispanicLatino, 3.5232\n",
      "\tBlueState, 0.4547\n",
      "\tmedian_age, 0.2424\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 0.1\n",
      "Features kept: 2, R-squared (training): 0.174, R-squared (test): 0.170\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\tLogHispanicLatino, 1.4446\n",
      "\tBlueState, 0.2799\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 1\n",
      "Features kept: 0, R-squared (training): 0.000, R-squared (test): -0.000\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Alpha = 5\n",
      "Features kept: 0, R-squared (training): 0.000, R-squared (test): -0.000\n",
      "\n",
      "Features with non-zero coefficients (sorted by absolute magnitude):\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "# List of alphas to try\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "# Loop over the alphas\n",
    "for alpha in alphas:\n",
    "    linlasso = Lasso(alpha=alpha, max_iter=10000, random_state=42).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    n_features = np.sum(linlasso.coef_ != 0)\n",
    "    \n",
    "    print('Alpha = {}\\nFeatures kept: {}, R-squared (training): {:.3f}, R-squared (test): {:.3f}\\n'\n",
    "          .format(alpha, n_features, r2_train, r2_test))\n",
    "    \n",
    "    # Print the coefficients for this alpha\n",
    "    print('Features with non-zero coefficients (sorted by absolute magnitude):')\n",
    "    coef_list = list(zip(features, linlasso.coef_))\n",
    "    non_zero_coefs = [e for e in coef_list if e[1] != 0]\n",
    "    sorted_coefs = sorted(non_zero_coefs, key=lambda e: -abs(e[1]))\n",
    "    for feature, coef in sorted_coefs:\n",
    "        print('\\t{}, {:.4f}'.format(feature, coef))\n",
    "    print('\\n' + '-'*60 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7799a3-549b-4743-a28b-708a2aff654a",
   "metadata": {},
   "source": [
    "2.c The criteria I will use to select the model is the one most parsimonous (the best bang for its buck model). This helps to pick a more general, model that is less likely to overfit, save computational time and have varaibles that are as MECE as possible.\n",
    "\n",
    "Picking Model Alpha = 0.01 with 3 features\n",
    "LogHispanicLatino, 3.5232\n",
    "\tBlueState, 0.4547\n",
    "\tmedian_age, 0.2424\n",
    " These are the only NON-zero features, the other 5 features are 0, which where the dummy varaibles for the day of week. \n",
    "\n",
    " The R^2 test is the the same as what we got for the Linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95b99b52-f277-4cf7-ac4e-0217be35779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "             Feature  Importance\n",
      "0  LogHispanicLatino    0.405580\n",
      "1          BlueState    0.036064\n",
      "2         median_age    0.004231\n",
      "3         day_Monday    0.000000\n",
      "4        day_Tuesday    0.000000\n"
     ]
    }
   ],
   "source": [
    "# ... (rest of your code)\n",
    "\n",
    "# Standardize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create DataFrames with original column names\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Train the Lasso model\n",
    "lasso_model = Lasso(alpha=0.01, random_state=42)\n",
    "lasso_model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(\n",
    "    lasso_model, X_test_scaled_df, y_test, n_repeats=10, random_state=42\n",
    ")\n",
    "\n",
    "# Get feature names and importance scores\n",
    "feature_names = X_test_scaled_df.columns\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print top 5 important features\n",
    "print(\"Top 5 Important Features:\")\n",
    "print(importance_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c33ac6-fde1-4896-b49d-174fa9564efc",
   "metadata": {},
   "source": [
    "2.c cont\n",
    "\n",
    "Note to TA: I couldnt get !pip scikit-learn==1.2.2 to work, I tried in many ways but its something to do with my Mac. I used a different approach here\n",
    "\n",
    "The results show that logHispanicLatino is the most importance compared to the rest of the variables. The next best 'Bluestate\" is higher than the rest but very close to 0 so really the LogHispanicLatino is the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88d223f9-a527-4402-b3a5-77c3383640c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Results:\n",
      "\n",
      "Alpha: 0.1\n",
      "Training R^2: 0.2705\n",
      "Test R^2: 0.2616\n",
      "Coefficients:\n",
      "  LogHispanicLatino: 0.5930\n",
      "  BlueState: 0.4531\n",
      "  median_age: 0.0567\n",
      "  day_Monday: -0.0335\n",
      "  day_Tuesday: -0.0174\n",
      "  day_Wednesday: -0.0288\n",
      "  day_Thursday: -0.0397\n",
      "  day_Saturday: 0.0258\n",
      "\n",
      "Alpha: 1\n",
      "Training R^2: 0.2705\n",
      "Test R^2: 0.2616\n",
      "Coefficients:\n",
      "  LogHispanicLatino: 0.5930\n",
      "  BlueState: 0.4530\n",
      "  median_age: 0.0567\n",
      "  day_Monday: -0.0335\n",
      "  day_Tuesday: -0.0173\n",
      "  day_Wednesday: -0.0288\n",
      "  day_Thursday: -0.0396\n",
      "  day_Saturday: 0.0258\n",
      "\n",
      "Alpha: 10\n",
      "Training R^2: 0.2705\n",
      "Test R^2: 0.2616\n",
      "Coefficients:\n",
      "  LogHispanicLatino: 0.5930\n",
      "  BlueState: 0.4525\n",
      "  median_age: 0.0568\n",
      "  day_Monday: -0.0333\n",
      "  day_Tuesday: -0.0172\n",
      "  day_Wednesday: -0.0287\n",
      "  day_Thursday: -0.0395\n",
      "  day_Saturday: 0.0259\n",
      "\n",
      "Most Appropriate Alpha:\n",
      "Alpha: 10\n",
      "Test R^2: 0.2616\n"
     ]
    }
   ],
   "source": [
    "# Define the penalty parameters to test\n",
    "alphas = [0.1, 1, 10]\n",
    "\n",
    "# Initialize results dictionary\n",
    "ridge_results = {\n",
    "    \"Alpha\": [],\n",
    "    \"Training R^2\": [],\n",
    "    \"Test R^2\": [],\n",
    "    \"Coefficients\": []\n",
    "}\n",
    "\n",
    "# Loop through each alpha value and fit Ridge regression\n",
    "for alpha in alphas:\n",
    "    # Initialize Ridge regression with the current alpha\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "\n",
    "    # Select the relevant features from the training and test sets\n",
    "    X_train_subset = X_train[features]\n",
    "    X_test_subset = X_test[features]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    ridge_model.fit(X_train_subset, y_train)\n",
    "\n",
    "    # Predict Y for training and test sets\n",
    "    y_train_pred = ridge_model.predict(X_train_subset)\n",
    "    y_test_pred = ridge_model.predict(X_test_subset)\n",
    "\n",
    "    # Calculate R-squared values\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Store results\n",
    "    ridge_results[\"Alpha\"].append(alpha)\n",
    "    ridge_results[\"Training R^2\"].append(train_r2)\n",
    "    ridge_results[\"Test R^2\"].append(test_r2)\n",
    "    ridge_results[\"Coefficients\"].append(ridge_model.coef_)\n",
    "\n",
    "# Display results\n",
    "print(\"Ridge Regression Results:\")\n",
    "for i, alpha in enumerate(ridge_results[\"Alpha\"]):\n",
    "    print(f\"\\nAlpha: {alpha}\")\n",
    "    print(f\"Training R^2: {ridge_results['Training R^2'][i]:.4f}\")\n",
    "    print(f\"Test R^2: {ridge_results['Test R^2'][i]:.4f}\")\n",
    "    print(\"Coefficients:\")\n",
    "    for feature, coef in zip(features, ridge_results[\"Coefficients\"][i]):\n",
    "        print(f\"  {feature}: {coef:.4f}\")\n",
    "\n",
    "# Choose the best alpha (highest Test R^2)\n",
    "best_alpha_idx = ridge_results[\"Test R^2\"].index(max(ridge_results[\"Test R^2\"]))\n",
    "print(\"\\nMost Appropriate Alpha:\")\n",
    "print(f\"Alpha: {ridge_results['Alpha'][best_alpha_idx]}\")\n",
    "print(f\"Test R^2: {ridge_results['Test R^2'][best_alpha_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349bf0f6-1b5d-445d-b328-bc757f450291",
   "metadata": {},
   "source": [
    "2.d\n",
    "\n",
    "I chose Alpha is 10 as it was a \"stricter\" model, it hadthe best test R^2 and it help to lower potentilly inflation results from the logHispanicLatino variable that it could have from any correlation with housing_units.\n",
    "\n",
    "     These features are very close to 0, similar to what we got using LASSO\n",
    "      day_Monday: -0.0333\n",
    "      day_Tuesday: -0.0172\n",
    "      day_Wednesday: -0.0287\n",
    "      day_Thursday: -0.0395\n",
    "      day_Saturday: 0.0259\n",
    "\n",
    " These features had more relevant coefficnets, but we see a big change in the coefficiant of LogHispanicLatino. The value got lower, this could mean the this model was stricter to penalize overfitting of that variable.\n",
    "  LogHispanicLatino: 0.5930\n",
    "  BlueState: 0.4525\n",
    "  median_age: 0.0568\n",
    "\n",
    "  The signs and coefficient values where almsot idential to the LASSO results and it confirms previous conclusions.\n",
    "\n",
    "  Test R^2: 0.2616 , it was slightly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5afe0fb0-515b-488d-98d9-0b06182a32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "             Feature  Importance\n",
      "0  LogHispanicLatino    0.431775\n",
      "1          BlueState    0.032096\n",
      "2         median_age    0.007856\n",
      "6       day_Thursday    0.000175\n",
      "3         day_Monday    0.000145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress the warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    result = permutation_importance(\n",
    "        ridge_model, X_test_scaled, y_test, n_repeats=30, random_state=42\n",
    "    )\n",
    "\n",
    "# Get feature names and importance scores\n",
    "feature_names = features\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print top 5 important features\n",
    "print(\"Top 5 Important Features:\")\n",
    "print(importance_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e6d82-e76f-48cc-b7f2-83795c877085",
   "metadata": {},
   "source": [
    "2.d cont Our results are similar to LASSO permutations, but the levle of importance scaled down by about a 1/3 on LogHispanicLatino and housing_units. But both models are consistent in ranking LogHispanicLatino, then BlueState then Median age in terms of importance.\n",
    "\n",
    "All 3 Models are good in figuring out which variables are signficant, but the linear regression may overstate the impact of LogHispanicLatino, and the use of penalty term helps reduce the variation inflation that may arise from that variable, and we see that the Ridge (alpha = 10) was the \"Stricktest\" in handled LogHispanicLatino.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
